import time
import openai
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

def test_semantic_chunking(transcript: str, similarity_threshold: float = 0.7) -> dict:
    """ì‹œë§¨í‹± ì²­í‚¹ í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ê° ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ë°˜í™˜"""
    
    # 1. ë¬¸ì¥ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸
    sentences = re.split(r'[.!?,]+', transcript)
    sentences = [s.strip() for s in sentences if len(s.strip()) >= 3]  # ìµœì†Œ 3ìë¡œ ë³€ê²½
    
    if len(sentences) <= 1:
        return {"error": "ë¬¸ì¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤"}
    
    # 2. ë¬¸ì¥ë³„ ì„ë² ë”© í…ŒìŠ¤íŠ¸
    embeddings = []
    valid_sentences = []
    
    for i, sentence in enumerate(sentences):
        try:
            time.sleep(0.1)
            embedding_response = openai.embeddings.create(
                input=sentence,
                model="text-embedding-3-small"
            )
            embedding = embedding_response.data[0].embedding
            embeddings.append(embedding)
            valid_sentences.append(sentence)
        except Exception as e:
            continue
    
    if len(embeddings) < 2:
        return {"error": "ì„ë² ë”©ëœ ë¬¸ì¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤"}
    
    # 3. ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    embeddings_array = np.array(embeddings)
    similarity_matrix = cosine_similarity(embeddings_array)
    
    # 4. í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸
    chunks_with_embeddings = []
    used_indices = set()
    
    for i in range(len(valid_sentences)):
        if i in used_indices:
            continue
            
        cluster_indices = [i]
        used_indices.add(i)
        
        for j in range(i + 1, len(valid_sentences)):
            if j in used_indices:
                continue
                
            if similarity_matrix[i][j] >= similarity_threshold:
                cluster_indices.append(j)
                used_indices.add(j)
        
        cluster_sentences = [valid_sentences[idx] for idx in cluster_indices]
        chunk_text = " ".join(cluster_sentences)
        
        if len(chunk_text.strip()) >= 20:
            chunk_embedding = embeddings[cluster_indices[0]]
            chunks_with_embeddings.append((chunk_text.strip(), chunk_embedding))
    
    # 5. ê²°ê³¼ ìš”ì•½
    result = {
        "total_sentences": len(sentences),
        "valid_sentences": len(valid_sentences),
        "embeddings_created": len(embeddings),
        "similarity_threshold": similarity_threshold,
        "chunks_created": len(chunks_with_embeddings),
        "chunks": []
    }
    
    for i, (chunk_text, embedding) in enumerate(chunks_with_embeddings):
        result["chunks"].append({
            "chunk_id": i+1,
            "text": chunk_text,
            "text_length": len(chunk_text),
            "embedding_dimension": len(embedding) if embedding else 0
        })
    
    return result


def load_cooking_transcript():
    """ìš”ë¦¬ ìë§‰ íŒŒì¼ì„ ì½ì–´ì„œ ë°˜í™˜"""
    try:
        with open('cooking_transcript.txt', 'r', encoding='utf-8') as f:
            transcript = f.read()
        print(f"ğŸ“– ìš”ë¦¬ ìë§‰ íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(transcript)}ì")
        return transcript
    except Exception as e:
        print(f"âŒ ìš”ë¦¬ ìë§‰ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        return None

def run_tests():
    """ìš”ë¦¬ ìë§‰ íŒŒì¼ ì‹œë§¨í‹± ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ìš”ë¦¬ ìë§‰ ì‹œë§¨í‹± ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ìš”ë¦¬ ìë§‰ íŒŒì¼ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: cooking_transcript (ì‹¤ì œ ìš”ë¦¬ ì˜ìƒ ìë§‰)")
    print("-" * 40)
    
    cooking_transcript = load_cooking_transcript()
    if cooking_transcript:
        try:
            result = test_semantic_chunking(cooking_transcript, similarity_threshold=0.5)  # 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶¤
            
            if "error" not in result:
                print(f"ğŸ“Š ìš”ë¦¬ ìë§‰ ì‹œë§¨í‹± ì²­í‚¹ ê²°ê³¼:")
                print(f"   ì´ ë¬¸ì¥ ìˆ˜: {result['total_sentences']}ê°œ")
                print(f"   ìƒì„±ëœ ì²­í¬ ìˆ˜: {result['chunks_created']}ê°œ")
                print(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {result['similarity_threshold']}")
                
                if result['chunks']:
                    print(f"\nğŸ¯ ê° ì²­í¬ë³„ ë¬¸ì¥ ìˆ˜:")
                    for chunk in result['chunks']:
                        # ì²­í¬ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•´ì„œ ì‹¤ì œ ë¬¸ì¥ ìˆ˜ ê³„ì‚°
                        chunk_sentences = re.split(r'[.!?,]+', chunk['text'])
                        chunk_sentences = [s.strip() for s in chunk_sentences if len(s.strip()) >= 3]
                        print(f"   ì²­í¬ {chunk['chunk_id']}: {len(chunk_sentences)}ê°œ ë¬¸ì¥")
            else:
                print(f"âŒ ìš”ë¦¬ ìë§‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
                
        except Exception as e:
            print(f"âŒ ìš”ë¦¬ ìë§‰ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        print("âŒ ìš”ë¦¬ ìë§‰ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    run_tests()